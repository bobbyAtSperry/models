{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n",
      "2016-12-28 23:17:05.623343: step 0, loss = 4.67 (18.1 examples/sec; 7.054 sec/batch)\n",
      "2016-12-28 23:17:15.127125: step 10, loss = 4.63 (145.9 examples/sec; 0.878 sec/batch)\n",
      "2016-12-28 23:17:23.886322: step 20, loss = 4.48 (152.4 examples/sec; 0.840 sec/batch)\n",
      "2016-12-28 23:17:32.935423: step 30, loss = 4.57 (140.2 examples/sec; 0.913 sec/batch)\n",
      "2016-12-28 23:17:42.157119: step 40, loss = 4.42 (148.8 examples/sec; 0.860 sec/batch)\n",
      "2016-12-28 23:17:50.886830: step 50, loss = 4.31 (149.9 examples/sec; 0.854 sec/batch)\n",
      "2016-12-28 23:18:00.008631: step 60, loss = 4.24 (136.1 examples/sec; 0.940 sec/batch)\n",
      "2016-12-28 23:18:08.893710: step 70, loss = 4.17 (140.5 examples/sec; 0.911 sec/batch)\n",
      "2016-12-28 23:18:18.151222: step 80, loss = 4.04 (145.4 examples/sec; 0.880 sec/batch)\n",
      "2016-12-28 23:18:27.445943: step 90, loss = 4.17 (137.1 examples/sec; 0.933 sec/batch)\n",
      "2016-12-28 23:18:37.511773: step 100, loss = 4.24 (112.2 examples/sec; 1.141 sec/batch)\n",
      "2016-12-28 23:18:47.369965: step 110, loss = 4.06 (142.3 examples/sec; 0.900 sec/batch)\n",
      "2016-12-28 23:18:56.498368: step 120, loss = 4.04 (116.0 examples/sec; 1.103 sec/batch)\n",
      "2016-12-28 23:19:05.657674: step 130, loss = 4.06 (138.3 examples/sec; 0.926 sec/batch)\n",
      "2016-12-28 23:19:14.340334: step 140, loss = 3.83 (144.8 examples/sec; 0.884 sec/batch)\n",
      "2016-12-28 23:19:23.696547: step 150, loss = 3.88 (122.2 examples/sec; 1.047 sec/batch)\n",
      "2016-12-28 23:19:34.676909: step 160, loss = 3.88 (110.7 examples/sec; 1.156 sec/batch)\n",
      "2016-12-28 23:19:44.050773: step 170, loss = 3.92 (142.6 examples/sec; 0.898 sec/batch)\n",
      "2016-12-28 23:19:54.137343: step 180, loss = 4.03 (117.7 examples/sec; 1.087 sec/batch)\n",
      "2016-12-28 23:20:04.514305: step 190, loss = 3.75 (147.2 examples/sec; 0.870 sec/batch)\n",
      "2016-12-28 23:20:18.072002: step 200, loss = 3.92 (44.2 examples/sec; 2.896 sec/batch)\n",
      "2016-12-28 23:20:31.130660: step 210, loss = 4.03 (145.9 examples/sec; 0.877 sec/batch)\n",
      "2016-12-28 23:20:39.954481: step 220, loss = 3.84 (150.9 examples/sec; 0.848 sec/batch)\n",
      "2016-12-28 23:20:48.509903: step 230, loss = 3.78 (153.4 examples/sec; 0.834 sec/batch)\n",
      "2016-12-28 23:20:57.661702: step 240, loss = 3.64 (132.3 examples/sec; 0.967 sec/batch)\n",
      "2016-12-28 23:21:08.692947: step 250, loss = 3.84 (136.3 examples/sec; 0.939 sec/batch)\n",
      "2016-12-28 23:21:19.287848: step 260, loss = 3.58 (132.2 examples/sec; 0.968 sec/batch)\n",
      "2016-12-28 23:21:30.286286: step 270, loss = 3.69 (119.7 examples/sec; 1.070 sec/batch)\n",
      "2016-12-28 23:21:41.063524: step 280, loss = 3.47 (99.5 examples/sec; 1.287 sec/batch)\n",
      "2016-12-28 23:21:52.695085: step 290, loss = 3.56 (106.2 examples/sec; 1.205 sec/batch)\n",
      "2016-12-28 23:22:02.351216: step 300, loss = 3.65 (113.1 examples/sec; 1.132 sec/batch)\n",
      "2016-12-28 23:22:11.900838: step 310, loss = 3.61 (137.1 examples/sec; 0.933 sec/batch)\n",
      "2016-12-28 23:22:21.521264: step 320, loss = 3.52 (109.9 examples/sec; 1.165 sec/batch)\n",
      "2016-12-28 23:22:30.684326: step 330, loss = 3.50 (148.3 examples/sec; 0.863 sec/batch)\n",
      "2016-12-28 23:22:39.721960: step 340, loss = 3.47 (150.8 examples/sec; 0.849 sec/batch)\n",
      "2016-12-28 23:22:48.766953: step 350, loss = 3.53 (137.7 examples/sec; 0.929 sec/batch)\n",
      "2016-12-28 23:22:58.262953: step 360, loss = 3.28 (148.2 examples/sec; 0.863 sec/batch)\n",
      "2016-12-28 23:23:07.585758: step 370, loss = 3.48 (133.3 examples/sec; 0.960 sec/batch)\n",
      "2016-12-28 23:23:16.880245: step 380, loss = 3.40 (140.7 examples/sec; 0.910 sec/batch)\n",
      "2016-12-28 23:23:25.762226: step 390, loss = 3.49 (126.2 examples/sec; 1.014 sec/batch)\n",
      "2016-12-28 23:23:35.233992: step 400, loss = 3.41 (122.8 examples/sec; 1.043 sec/batch)\n",
      "2016-12-28 23:23:45.097980: step 410, loss = 3.64 (135.5 examples/sec; 0.945 sec/batch)\n",
      "2016-12-28 23:23:55.436140: step 420, loss = 3.35 (126.7 examples/sec; 1.010 sec/batch)\n",
      "2016-12-28 23:24:04.454498: step 430, loss = 3.52 (141.0 examples/sec; 0.908 sec/batch)\n",
      "2016-12-28 23:24:13.364331: step 440, loss = 3.42 (131.2 examples/sec; 0.976 sec/batch)\n",
      "2016-12-28 23:24:22.729551: step 450, loss = 3.28 (137.2 examples/sec; 0.933 sec/batch)\n",
      "2016-12-28 23:24:31.841976: step 460, loss = 3.13 (130.9 examples/sec; 0.978 sec/batch)\n",
      "2016-12-28 23:24:40.883824: step 470, loss = 3.22 (149.7 examples/sec; 0.855 sec/batch)\n",
      "2016-12-28 23:24:50.018533: step 480, loss = 3.23 (132.7 examples/sec; 0.965 sec/batch)\n",
      "2016-12-28 23:24:59.283084: step 490, loss = 3.14 (135.5 examples/sec; 0.944 sec/batch)\n",
      "2016-12-28 23:25:09.167799: step 500, loss = 3.09 (104.9 examples/sec; 1.220 sec/batch)\n",
      "2016-12-28 23:25:19.333599: step 510, loss = 3.16 (108.6 examples/sec; 1.178 sec/batch)\n",
      "2016-12-28 23:25:28.419750: step 520, loss = 3.16 (148.3 examples/sec; 0.863 sec/batch)\n",
      "2016-12-28 23:25:37.663732: step 530, loss = 3.10 (127.1 examples/sec; 1.007 sec/batch)\n",
      "2016-12-28 23:25:46.592309: step 540, loss = 3.34 (149.2 examples/sec; 0.858 sec/batch)\n",
      "2016-12-28 23:25:55.288001: step 550, loss = 3.10 (144.9 examples/sec; 0.883 sec/batch)\n",
      "2016-12-28 23:26:04.039919: step 560, loss = 2.95 (135.4 examples/sec; 0.945 sec/batch)\n",
      "2016-12-28 23:26:12.879876: step 570, loss = 3.11 (149.6 examples/sec; 0.856 sec/batch)\n",
      "2016-12-28 23:26:21.704063: step 580, loss = 3.13 (134.9 examples/sec; 0.949 sec/batch)\n",
      "2016-12-28 23:26:30.649386: step 590, loss = 3.16 (145.0 examples/sec; 0.883 sec/batch)\n",
      "2016-12-28 23:26:39.428621: step 600, loss = 3.10 (121.5 examples/sec; 1.054 sec/batch)\n",
      "2016-12-28 23:26:48.779715: step 610, loss = 2.93 (148.9 examples/sec; 0.860 sec/batch)\n",
      "2016-12-28 23:26:57.401123: step 620, loss = 2.90 (134.4 examples/sec; 0.952 sec/batch)\n",
      "2016-12-28 23:27:06.001492: step 630, loss = 3.18 (151.9 examples/sec; 0.843 sec/batch)\n",
      "2016-12-28 23:27:15.165287: step 640, loss = 2.93 (152.8 examples/sec; 0.838 sec/batch)\n",
      "2016-12-28 23:27:23.645638: step 650, loss = 2.84 (149.7 examples/sec; 0.855 sec/batch)\n",
      "2016-12-28 23:27:32.537586: step 660, loss = 2.93 (146.7 examples/sec; 0.872 sec/batch)\n",
      "2016-12-28 23:27:41.632520: step 670, loss = 2.91 (149.6 examples/sec; 0.856 sec/batch)\n",
      "2016-12-28 23:27:50.791041: step 680, loss = 2.81 (135.6 examples/sec; 0.944 sec/batch)\n",
      "2016-12-28 23:27:59.472211: step 690, loss = 2.81 (151.7 examples/sec; 0.844 sec/batch)\n",
      "2016-12-28 23:28:08.276030: step 700, loss = 2.88 (119.8 examples/sec; 1.068 sec/batch)\n",
      "2016-12-28 23:28:16.835704: step 710, loss = 2.95 (154.5 examples/sec; 0.829 sec/batch)\n",
      "2016-12-28 23:28:25.391764: step 720, loss = 2.92 (148.0 examples/sec; 0.865 sec/batch)\n",
      "2016-12-28 23:28:33.915084: step 730, loss = 3.26 (152.5 examples/sec; 0.839 sec/batch)\n",
      "2016-12-28 23:28:42.955054: step 740, loss = 2.80 (153.8 examples/sec; 0.832 sec/batch)\n",
      "2016-12-28 23:28:51.438459: step 750, loss = 2.72 (151.1 examples/sec; 0.847 sec/batch)\n",
      "2016-12-28 23:29:00.568512: step 760, loss = 2.73 (146.9 examples/sec; 0.871 sec/batch)\n",
      "2016-12-28 23:29:09.198571: step 770, loss = 2.82 (148.9 examples/sec; 0.860 sec/batch)\n",
      "2016-12-28 23:29:17.748644: step 780, loss = 2.67 (149.9 examples/sec; 0.854 sec/batch)\n",
      "2016-12-28 23:29:26.670709: step 790, loss = 2.75 (151.5 examples/sec; 0.845 sec/batch)\n",
      "2016-12-28 23:29:35.453556: step 800, loss = 2.73 (122.9 examples/sec; 1.041 sec/batch)\n",
      "2016-12-28 23:29:44.772109: step 810, loss = 2.53 (138.8 examples/sec; 0.922 sec/batch)\n",
      "2016-12-28 23:29:54.220401: step 820, loss = 2.63 (127.4 examples/sec; 1.005 sec/batch)\n",
      "2016-12-28 23:30:02.869468: step 830, loss = 2.68 (143.4 examples/sec; 0.893 sec/batch)\n",
      "2016-12-28 23:30:11.986474: step 840, loss = 2.88 (114.8 examples/sec; 1.115 sec/batch)\n",
      "2016-12-28 23:30:24.214285: step 850, loss = 2.56 (123.4 examples/sec; 1.037 sec/batch)\n",
      "2016-12-28 23:30:33.928392: step 860, loss = 2.77 (109.5 examples/sec; 1.169 sec/batch)\n",
      "2016-12-28 23:30:42.432390: step 870, loss = 2.64 (152.4 examples/sec; 0.840 sec/batch)\n",
      "2016-12-28 23:30:51.089117: step 880, loss = 2.56 (140.7 examples/sec; 0.910 sec/batch)\n",
      "2016-12-28 23:31:00.031092: step 890, loss = 3.18 (147.5 examples/sec; 0.868 sec/batch)\n",
      "2016-12-28 23:31:08.894306: step 900, loss = 2.49 (113.9 examples/sec; 1.124 sec/batch)\n",
      "2016-12-28 23:31:17.867772: step 910, loss = 2.69 (156.5 examples/sec; 0.818 sec/batch)\n",
      "2016-12-28 23:31:26.347542: step 920, loss = 2.57 (149.1 examples/sec; 0.858 sec/batch)\n",
      "2016-12-28 23:31:35.061108: step 930, loss = 2.59 (142.2 examples/sec; 0.900 sec/batch)\n",
      "2016-12-28 23:31:44.187557: step 940, loss = 2.75 (149.2 examples/sec; 0.858 sec/batch)\n",
      "2016-12-28 23:31:53.006823: step 950, loss = 2.40 (148.3 examples/sec; 0.863 sec/batch)\n",
      "2016-12-28 23:32:01.976592: step 960, loss = 2.55 (152.4 examples/sec; 0.840 sec/batch)\n",
      "2016-12-28 23:32:10.954677: step 970, loss = 2.34 (141.4 examples/sec; 0.905 sec/batch)\n",
      "2016-12-28 23:32:20.299101: step 980, loss = 2.29 (148.9 examples/sec; 0.860 sec/batch)\n",
      "2016-12-28 23:32:29.130402: step 990, loss = 2.45 (117.1 examples/sec; 1.093 sec/batch)\n",
      "2016-12-28 23:32:38.050372: step 1000, loss = 2.59 (124.5 examples/sec; 1.029 sec/batch)\n",
      "2016-12-28 23:32:48.143123: step 1010, loss = 2.53 (145.7 examples/sec; 0.879 sec/batch)\n",
      "2016-12-28 23:32:58.407462: step 1020, loss = 2.35 (131.1 examples/sec; 0.976 sec/batch)\n",
      "2016-12-28 23:33:08.785418: step 1030, loss = 2.38 (130.8 examples/sec; 0.979 sec/batch)\n",
      "2016-12-28 23:33:18.394805: step 1040, loss = 2.31 (113.2 examples/sec; 1.131 sec/batch)\n",
      "2016-12-28 23:33:27.387057: step 1050, loss = 2.47 (153.4 examples/sec; 0.834 sec/batch)\n",
      "2016-12-28 23:33:36.087590: step 1060, loss = 2.39 (150.2 examples/sec; 0.852 sec/batch)\n",
      "2016-12-28 23:33:44.587468: step 1070, loss = 2.33 (150.9 examples/sec; 0.848 sec/batch)\n",
      "2016-12-28 23:33:53.116403: step 1080, loss = 2.15 (150.4 examples/sec; 0.851 sec/batch)\n",
      "2016-12-28 23:34:01.940309: step 1090, loss = 2.25 (149.1 examples/sec; 0.858 sec/batch)\n",
      "2016-12-28 23:34:11.248127: step 1100, loss = 2.21 (109.0 examples/sec; 1.174 sec/batch)\n",
      "2016-12-28 23:34:20.336032: step 1110, loss = 2.27 (145.7 examples/sec; 0.879 sec/batch)\n",
      "2016-12-28 23:34:29.709867: step 1120, loss = 2.43 (148.7 examples/sec; 0.861 sec/batch)\n",
      "2016-12-28 23:34:38.393603: step 1130, loss = 2.43 (149.0 examples/sec; 0.859 sec/batch)\n",
      "2016-12-28 23:34:46.954052: step 1140, loss = 2.45 (152.6 examples/sec; 0.839 sec/batch)\n",
      "2016-12-28 23:34:55.933663: step 1150, loss = 2.16 (148.0 examples/sec; 0.865 sec/batch)\n",
      "2016-12-28 23:35:04.896329: step 1160, loss = 2.38 (150.4 examples/sec; 0.851 sec/batch)\n",
      "2016-12-28 23:35:14.143202: step 1170, loss = 2.35 (144.4 examples/sec; 0.886 sec/batch)\n",
      "2016-12-28 23:35:23.234413: step 1180, loss = 2.30 (150.0 examples/sec; 0.853 sec/batch)\n",
      "2016-12-28 23:35:33.239761: step 1190, loss = 2.40 (111.7 examples/sec; 1.145 sec/batch)\n",
      "2016-12-28 23:35:42.929945: step 1200, loss = 2.23 (94.7 examples/sec; 1.352 sec/batch)\n",
      "2016-12-28 23:35:51.751153: step 1210, loss = 2.43 (142.5 examples/sec; 0.898 sec/batch)\n",
      "2016-12-28 23:36:00.287970: step 1220, loss = 2.18 (145.8 examples/sec; 0.878 sec/batch)\n",
      "2016-12-28 23:36:09.146089: step 1230, loss = 2.05 (142.7 examples/sec; 0.897 sec/batch)\n",
      "2016-12-28 23:36:18.060008: step 1240, loss = 2.04 (150.5 examples/sec; 0.851 sec/batch)\n",
      "2016-12-28 23:36:28.210501: step 1250, loss = 2.41 (147.4 examples/sec; 0.868 sec/batch)\n",
      "2016-12-28 23:36:38.917093: step 1260, loss = 2.25 (143.4 examples/sec; 0.893 sec/batch)\n",
      "2016-12-28 23:36:48.264468: step 1270, loss = 2.34 (148.7 examples/sec; 0.861 sec/batch)\n",
      "2016-12-28 23:36:57.479306: step 1280, loss = 2.27 (118.4 examples/sec; 1.081 sec/batch)\n",
      "2016-12-28 23:37:06.231959: step 1290, loss = 2.13 (153.1 examples/sec; 0.836 sec/batch)\n",
      "2016-12-28 23:37:15.834391: step 1300, loss = 2.13 (123.2 examples/sec; 1.039 sec/batch)\n",
      "2016-12-28 23:37:25.074855: step 1310, loss = 2.08 (134.9 examples/sec; 0.949 sec/batch)\n",
      "2016-12-28 23:37:35.935282: step 1320, loss = 2.24 (151.5 examples/sec; 0.845 sec/batch)\n",
      "2016-12-28 23:37:45.345193: step 1330, loss = 2.18 (142.2 examples/sec; 0.900 sec/batch)\n",
      "2016-12-28 23:37:54.287189: step 1340, loss = 1.87 (145.9 examples/sec; 0.877 sec/batch)\n",
      "2016-12-28 23:38:03.746843: step 1350, loss = 1.97 (135.6 examples/sec; 0.944 sec/batch)\n",
      "2016-12-28 23:38:14.276138: step 1360, loss = 1.94 (109.2 examples/sec; 1.172 sec/batch)\n",
      "2016-12-28 23:38:23.817945: step 1370, loss = 2.15 (143.6 examples/sec; 0.891 sec/batch)\n",
      "2016-12-28 23:38:33.739988: step 1380, loss = 2.08 (128.3 examples/sec; 0.998 sec/batch)\n",
      "2016-12-28 23:38:42.962721: step 1390, loss = 1.97 (155.1 examples/sec; 0.825 sec/batch)\n",
      "2016-12-28 23:38:52.061561: step 1400, loss = 2.26 (115.9 examples/sec; 1.104 sec/batch)\n",
      "2016-12-28 23:39:00.569519: step 1410, loss = 2.09 (149.9 examples/sec; 0.854 sec/batch)\n",
      "2016-12-28 23:39:09.268653: step 1420, loss = 2.38 (117.1 examples/sec; 1.093 sec/batch)\n",
      "2016-12-28 23:39:18.530574: step 1430, loss = 2.07 (152.0 examples/sec; 0.842 sec/batch)\n",
      "2016-12-28 23:39:27.888689: step 1440, loss = 1.92 (125.1 examples/sec; 1.023 sec/batch)\n",
      "2016-12-28 23:39:37.345421: step 1450, loss = 1.94 (130.8 examples/sec; 0.978 sec/batch)\n",
      "2016-12-28 23:39:47.138439: step 1460, loss = 1.99 (154.1 examples/sec; 0.831 sec/batch)\n",
      "2016-12-28 23:39:56.719401: step 1470, loss = 2.09 (120.4 examples/sec; 1.063 sec/batch)\n",
      "2016-12-28 23:40:05.585213: step 1480, loss = 1.91 (152.0 examples/sec; 0.842 sec/batch)\n",
      "2016-12-28 23:40:17.327482: step 1490, loss = 2.05 (73.5 examples/sec; 1.742 sec/batch)\n",
      "2016-12-28 23:40:28.639614: step 1500, loss = 1.90 (107.7 examples/sec; 1.188 sec/batch)\n",
      "2016-12-28 23:40:38.444794: step 1510, loss = 1.77 (152.8 examples/sec; 0.838 sec/batch)\n",
      "2016-12-28 23:40:48.008639: step 1520, loss = 2.05 (109.7 examples/sec; 1.167 sec/batch)\n",
      "2016-12-28 23:40:58.035622: step 1530, loss = 2.04 (135.6 examples/sec; 0.944 sec/batch)\n",
      "2016-12-28 23:41:06.923940: step 1540, loss = 1.88 (155.9 examples/sec; 0.821 sec/batch)\n",
      "2016-12-28 23:41:15.459667: step 1550, loss = 2.06 (154.6 examples/sec; 0.828 sec/batch)\n",
      "2016-12-28 23:41:23.889310: step 1560, loss = 2.03 (155.0 examples/sec; 0.826 sec/batch)\n",
      "2016-12-28 23:41:32.404868: step 1570, loss = 1.95 (155.6 examples/sec; 0.823 sec/batch)\n",
      "2016-12-28 23:41:41.426208: step 1580, loss = 1.81 (148.4 examples/sec; 0.863 sec/batch)\n",
      "2016-12-28 23:41:50.011897: step 1590, loss = 1.84 (138.1 examples/sec; 0.927 sec/batch)\n",
      "2016-12-28 23:41:59.491373: step 1600, loss = 1.98 (114.5 examples/sec; 1.118 sec/batch)\n",
      "2016-12-28 23:42:09.511619: step 1610, loss = 1.71 (108.8 examples/sec; 1.177 sec/batch)\n",
      "2016-12-28 23:42:18.644781: step 1620, loss = 1.81 (145.8 examples/sec; 0.878 sec/batch)\n",
      "2016-12-28 23:42:27.609196: step 1630, loss = 1.88 (101.6 examples/sec; 1.260 sec/batch)\n",
      "2016-12-28 23:42:36.345078: step 1640, loss = 1.92 (134.6 examples/sec; 0.951 sec/batch)\n",
      "2016-12-28 23:42:45.427549: step 1650, loss = 1.83 (139.1 examples/sec; 0.920 sec/batch)\n",
      "2016-12-28 23:42:54.267192: step 1660, loss = 1.96 (155.4 examples/sec; 0.824 sec/batch)\n",
      "2016-12-28 23:43:03.384088: step 1670, loss = 1.81 (131.7 examples/sec; 0.972 sec/batch)\n",
      "2016-12-28 23:43:13.423499: step 1680, loss = 1.92 (131.9 examples/sec; 0.970 sec/batch)\n",
      "2016-12-28 23:43:22.462465: step 1690, loss = 1.89 (136.2 examples/sec; 0.940 sec/batch)\n",
      "2016-12-28 23:43:31.098862: step 1700, loss = 1.67 (130.0 examples/sec; 0.984 sec/batch)\n",
      "2016-12-28 23:43:39.472640: step 1710, loss = 1.77 (155.3 examples/sec; 0.824 sec/batch)\n",
      "2016-12-28 23:43:48.194590: step 1720, loss = 1.83 (154.9 examples/sec; 0.826 sec/batch)\n",
      "2016-12-28 23:43:56.597194: step 1730, loss = 1.68 (155.0 examples/sec; 0.826 sec/batch)\n",
      "2016-12-28 23:44:05.028809: step 1740, loss = 1.86 (149.3 examples/sec; 0.858 sec/batch)\n",
      "2016-12-28 23:44:14.075672: step 1750, loss = 1.77 (153.9 examples/sec; 0.832 sec/batch)\n",
      "2016-12-28 23:44:22.968584: step 1760, loss = 1.75 (150.8 examples/sec; 0.849 sec/batch)\n",
      "2016-12-28 23:44:31.802415: step 1770, loss = 1.78 (150.8 examples/sec; 0.849 sec/batch)\n",
      "2016-12-28 23:44:40.432351: step 1780, loss = 1.72 (150.2 examples/sec; 0.852 sec/batch)\n",
      "2016-12-28 23:44:48.986336: step 1790, loss = 1.77 (144.7 examples/sec; 0.884 sec/batch)\n",
      "2016-12-28 23:44:59.867133: step 1800, loss = 1.90 (82.8 examples/sec; 1.545 sec/batch)\n",
      "2016-12-28 23:45:10.904705: step 1810, loss = 1.79 (143.5 examples/sec; 0.892 sec/batch)\n",
      "2016-12-28 23:45:21.560667: step 1820, loss = 1.64 (140.6 examples/sec; 0.911 sec/batch)\n",
      "2016-12-28 23:45:30.757179: step 1830, loss = 1.76 (132.5 examples/sec; 0.966 sec/batch)\n",
      "2016-12-28 23:45:39.376348: step 1840, loss = 1.71 (146.1 examples/sec; 0.876 sec/batch)\n",
      "2016-12-28 23:45:48.341976: step 1850, loss = 1.67 (128.9 examples/sec; 0.993 sec/batch)\n",
      "2016-12-28 23:45:56.806151: step 1860, loss = 1.73 (150.2 examples/sec; 0.852 sec/batch)\n",
      "2016-12-28 23:46:06.140054: step 1870, loss = 1.68 (126.3 examples/sec; 1.013 sec/batch)\n",
      "2016-12-28 23:46:16.331856: step 1880, loss = 1.58 (131.6 examples/sec; 0.973 sec/batch)\n",
      "2016-12-28 23:46:25.243395: step 1890, loss = 1.70 (149.8 examples/sec; 0.854 sec/batch)\n",
      "2016-12-28 23:46:34.872567: step 1900, loss = 1.59 (110.2 examples/sec; 1.161 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "!python cifar10_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cifar10_train\n",
    "cifar10_train.tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-12-28 23:14:07.450231: precision @ 1 = 0.766\r\n"
     ]
    }
   ],
   "source": [
    "!python cifar10_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
